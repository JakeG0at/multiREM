{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUPREME is setting up!\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Description: Training script for SUPREME adapted to MultiREM project \n",
    "\n",
    "TODO: \n",
    "- Add mlflow for run tracking \n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "# SUPREME run\n",
    "print('SUPREME is setting up!')\n",
    "\n",
    "from lib.model import Net, train_epoch, validate_epoch\n",
    "# from lib.config import Config \n",
    "\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import itertools\n",
    "import pickle5 as pickle\n",
    "import argparse\n",
    "import errno\n",
    "import warnings\n",
    "import logging\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import statistics\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, train_test_split, RandomizedSearchCV, GridSearchCV\n",
    "\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# TODO: rpy2 doesn't work on m1 macs -> need to find a way around this \n",
    "# import rpy2\n",
    "# import rpy2.robjects as robjects\n",
    "# from rpy2.robjects.packages import importr\n",
    "# utils = importr('utils')\n",
    "# rFerns = importr('rFerns')\n",
    "# Boruta = importr('Boruta')\n",
    "# pracma = importr('pracma')\n",
    "# dplyr = importr('dplyr')\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOBALS\n",
    "# DEVICE = torch.device('cuda' if torch.cuda.is_available() else (torch.device('mps') if torch.has_mps else 'cpu'))\n",
    "DEVICE = torch.device('cpu')\n",
    "NETWORKS = ['exp', 'cna']\n",
    "DATA_DIR = '../data/sample_data'\n",
    "OUTPUT_DIR = './output'\n",
    "\n",
    "NODE_FEATURE_SELECTION = False # Reduce dimensionality of node vectors (TODO make this part of config)\n",
    "NODE_NUM_FEATURES = [50] * len(NETWORKS)\n",
    "NUM_BORUTA_RUNS = 100 # Number of Boruta runs to perform for feature selection (TODO make this part of config)\n",
    "\n",
    "MAX_EPOCHS = 500\n",
    "MIN_EPOCHS = 200\n",
    "PATIENCE = 30\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTIONS\n",
    "def get_data(dir, networks): \n",
    "    labels = pd.read_pickle(os.path.join(dir, 'labels.pkl'))\n",
    "    nodes = {}\n",
    "    edges = {}\n",
    "    for n in networks: \n",
    "        nodes[n] = pd.read_pickle(os.path.join(dir, f'{n}.pkl'))\n",
    "        edges[n] = pd.read_pickle(os.path.join(dir, f'edges_{n}.pkl'))\n",
    "\n",
    "    return labels, nodes, edges\n",
    "\n",
    "    \n",
    "\n",
    "def select_node_features(feat, num_features, boruta_runs, device): \n",
    "    # Boruta feature selection to reduce dimensionality of node vectors \n",
    "    # TODO: This doesn't work on ARM for M1 macs because of some R dependency -> migrate to python\n",
    "\n",
    "    feat_flat = [item for sublist in feat.values.tolist() for item in sublist]\n",
    "    feat_temp = robjects.FloatVector(feat_flat)\n",
    "    robjects.globalenv['feat_matrix'] = robjects.r('matrix')(feat_temp)\n",
    "    robjects.globalenv['feat_x'] = robjects.IntVector(feat.shape)\n",
    "    robjects.globalenv['labels_vector'] = robjects.IntVector(labels.tolist())\n",
    "    robjects.globalenv['top'] = num_features\n",
    "    robjects.globalenv['maxBorutaRuns'] = boruta_runs\n",
    "    robjects.r('''\n",
    "        require(rFerns)\n",
    "        require(Boruta)\n",
    "        labels_vector = as.factor(labels_vector)\n",
    "        feat_matrix <- Reshape(feat_matrix, feat_x[1])\n",
    "        feat_data = data.frame(feat_matrix)\n",
    "        colnames(feat_data) <- 1:feat_x[2]\n",
    "        feat_data <- feat_data %>%\n",
    "            mutate('Labels' = labels_vector)\n",
    "        boruta.train <- Boruta(feat_data$Labels ~ ., data= feat_data, doTrace = 0, getImp=getImpFerns, holdHistory = T, maxRuns = maxBorutaRuns)\n",
    "        thr = sort(attStats(boruta.train)$medianImp, decreasing = T)[top]\n",
    "        boruta_signif = rownames(attStats(boruta.train)[attStats(boruta.train)$medianImp >= thr,])\n",
    "            ''')\n",
    "    boruta_signif = robjects.globalenv['boruta_signif']\n",
    "    robjects.r.rm(\"feat_matrix\")\n",
    "    robjects.r.rm(\"labels_vector\")\n",
    "    robjects.r.rm(\"feat_data\")\n",
    "    robjects.r.rm(\"boruta_signif\")\n",
    "    robjects.r.rm(\"thr\")\n",
    "    topx = []\n",
    "    for index in boruta_signif:\n",
    "        t_index=re.sub(\"`\",\"\",index)\n",
    "        topx.append((np.array(feat.values).T)[int(t_index)-1])\n",
    "    topx = np.array(topx)\n",
    "    values = torch.tensor(topx.T, device=device)\n",
    "\n",
    "    return values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 5\n"
     ]
    }
   ],
   "source": [
    "labels, nodes, edges = get_data(DATA_DIR, NETWORKS)\n",
    "num_classes = torch.unique(labels).shape[0]\n",
    "print(f'Number of classes: {num_classes}')\n",
    "\n",
    "train_valid_idx, test_idx = train_test_split(np.arange(len(labels)), test_size=0.20, shuffle=True, stratify=labels, random_state=42)\n",
    "\n",
    "# Run feature selection if desired \n",
    "if NODE_FEATURE_SELECTION:\n",
    "    X_nodes = {}\n",
    "    for n in NETWORKS:\n",
    "        # TODO: This doesn't work \n",
    "        X_nodes[n] = select_node_features(nodes[n], NODE_NUM_FEATURES[NETWORKS.index(n)], NUM_BORUTA_RUNS, DEVICE)\n",
    "else:\n",
    "    X_nodes = nodes\n",
    "\n",
    "X_nodes_cat = torch.cat([torch.tensor(v.values).float() for v in X_nodes.values()], dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# For each network \n",
    "# for n in NETWORKS: \n",
    "n = NETWORKS[0]\n",
    "\n",
    "# For each hyperparam combo\n",
    "learning_rate = 0.01\n",
    "hid_size = 32\n",
    "best_valid_loss = np.Inf\n",
    "\n",
    "\n",
    "\n",
    "# Build a graph where node features have all node feature datatypes concatenated\n",
    "#  - The edges are still just for this network \n",
    "edge_index = edges[n]\n",
    "data = Data(x=X_nodes_cat, \n",
    "            edge_index=torch.tensor(edge_index[edge_index.columns[0:2]].transpose().values, device=DEVICE).long(),\n",
    "            edge_attr=torch.tensor(edge_index[edge_index.columns[2]].transpose().values, device=DEVICE).float(), \n",
    "            y=labels) \n",
    "\n",
    "# Split data into training and validation sets\n",
    "X = data.x[train_valid_idx]\n",
    "y = data.y[train_valid_idx]\n",
    "\n",
    "# For each split \n",
    "rskf = RepeatedStratifiedKFold(n_splits=10, n_repeats=1)\n",
    "valid_losses = []\n",
    "for train_part, valid_part in rskf.split(X, y):\n",
    "    train_idx = train_valid_idx[train_part]\n",
    "    valid_idx = train_valid_idx[valid_part]\n",
    "\n",
    "    # Get train/valid split \n",
    "    data.train_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "    data.train_mask[train_idx] = True\n",
    "    data.valid_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "    data.valid_mask[valid_idx] = True\n",
    "\n",
    "    # Build model \n",
    "    model = Net(in_size=data.x.shape[1], hid_size=hid_size, out_size=num_classes).to(DEVICE)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # Train model \n",
    "    min_valid_loss = np.Inf\n",
    "    patience_count = 0\n",
    "\n",
    "    for epoch in range(MAX_EPOCHS):\n",
    "        emb = train_epoch(model, data, optimizer, criterion)\n",
    "        curr_valid_loss, emb = validate_epoch(model, data, criterion)\n",
    "\n",
    "        if curr_valid_loss < min_valid_loss:\n",
    "            min_valid_loss = curr_valid_loss\n",
    "            patience_count = 0\n",
    "        else:\n",
    "            patience_count += 1\n",
    "\n",
    "        # If model isn't learning anything, break\n",
    "        if epoch >= MIN_EPOCHS and patience_count >= PATIENCE:\n",
    "            break\n",
    "    \n",
    "    valid_losses.append(min_valid_loss.item())\n",
    "\n",
    "\n",
    "curr_valid_loss_overall = np.mean(valid_losses)            \n",
    "if curr_valid_loss_overall < best_valid_loss:\n",
    "    best_valid_loss = curr_valid_loss_overall\n",
    "    best_emb_lr = learning_rate\n",
    "    best_emb_hs = hid_size\n",
    "\n",
    "\n",
    "# Retrain the model on the entire training set with the best hyperparams\n",
    "data = Data(x=X_nodes_cat, edge_index=torch.tensor(edge_index[edge_index.columns[0:2]].transpose().values, device=DEVICE).long(),\n",
    "            edge_attr=torch.tensor(edge_index[edge_index.columns[2]].transpose().values, device=DEVICE).float(), y=labels) \n",
    "X = data.x[train_valid_idx]\n",
    "y = data.y[train_valid_idx]\n",
    "\n",
    "data.train_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "data.train_mask[train_valid_idx] = True\n",
    "data.valid_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "data.valid_mask[test_idx] = True\n",
    "\n",
    "model = Net(in_size=data.x.shape[1], hid_size=best_emb_hs, out_size=num_classes).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=best_emb_lr)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "selected_emb = None\n",
    "\n",
    "min_valid_loss = np.Inf\n",
    "patience_count = 0\n",
    "            \n",
    "for epoch in range(MAX_EPOCHS):\n",
    "    emb = train_epoch(model, data, optimizer, criterion)\n",
    "    this_valid_loss, emb = validate_epoch(model, data, criterion)\n",
    "\n",
    "    if this_valid_loss < min_valid_loss:\n",
    "        min_valid_loss = this_valid_loss\n",
    "        patience_count = 0\n",
    "        selected_emb = emb\n",
    "    else:\n",
    "        patience_count += 1\n",
    "\n",
    "    if epoch >= MIN_EPOCHS and patience_count >= PATIENCE:\n",
    "        break\n",
    "\n",
    "# Save the embeddings \n",
    "emb_file = os.path.join(OUTPUT_DIR, f\"{n}_embeddings.pkl\")\n",
    "with open(emb_file, 'wb') as f:\n",
    "    pickle.dump(selected_emb, f)\n",
    "    pd.DataFrame(selected_emb).to_csv(emb_file[:-4] + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multirem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
