{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUPREME is setting up!\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Description: Training script for SUPREME adapted to MultiREM project \n",
    "\n",
    "TODO: \n",
    "- Add mlflow for run tracking \n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "# SUPREME run\n",
    "print('SUPREME is setting up!')\n",
    "\n",
    "from lib.model import Net, train_epoch, validate_epoch\n",
    "# from lib.config import Config \n",
    "\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import itertools\n",
    "import pickle5 as pickle\n",
    "import argparse\n",
    "import errno\n",
    "import warnings\n",
    "import logging\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import statistics\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, train_test_split, RandomizedSearchCV, GridSearchCV\n",
    "\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "# TODO: rpy2 doesn't work on m1 macs -> need to find a way around this \n",
    "# import rpy2\n",
    "# import rpy2.robjects as robjects\n",
    "# from rpy2.robjects.packages import importr\n",
    "# utils = importr('utils')\n",
    "# rFerns = importr('rFerns')\n",
    "# Boruta = importr('Boruta')\n",
    "# pracma = importr('pracma')\n",
    "# dplyr = importr('dplyr')\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOBALS\n",
    "# DEVICE = torch.device('cuda' if torch.cuda.is_available() else (torch.device('mps') if torch.has_mps else 'cpu'))\n",
    "DEVICE = torch.device('cpu')\n",
    "NETWORKS = ['exp', 'cna']\n",
    "DATA_DIR = '../data/sample_data'\n",
    "OUTPUT_DIR = './output'\n",
    "\n",
    "NODE_FEATURE_SELECTION = False # Reduce dimensionality of node vectors (TODO make this part of config)\n",
    "NODE_NUM_FEATURES = [50] * len(NETWORKS)\n",
    "NUM_BORUTA_RUNS = 100 # Number of Boruta runs to perform for feature selection (TODO make this part of config)\n",
    "\n",
    "MAX_EPOCHS = 500\n",
    "MIN_EPOCHS = 200\n",
    "PATIENCE = 30\n",
    "\n",
    "HIDDEN_LAYER_SIZES = [50, 100, 200]\n",
    "LEARNING_RATES = [0.01, 0.001, 0.0001]\n",
    "\n",
    "ADD_RAW_FEATURES = True\n",
    "\n",
    "CLASSIFIER = \"MLP\"\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTIONS\n",
    "def get_data(dir, networks): \n",
    "    labels = pd.read_pickle(os.path.join(dir, 'labels.pkl'))\n",
    "    nodes = {}\n",
    "    edges = {}\n",
    "    for n in networks: \n",
    "        nodes[n] = pd.read_pickle(os.path.join(dir, f'{n}.pkl'))\n",
    "        edges[n] = pd.read_pickle(os.path.join(dir, f'edges_{n}.pkl'))\n",
    "\n",
    "    return labels, nodes, edges\n",
    "\n",
    "    \n",
    "\n",
    "def select_node_features(feat, num_features, boruta_runs, device): \n",
    "    # Boruta feature selection to reduce dimensionality of node vectors \n",
    "    # TODO: This doesn't work on ARM for M1 macs because of some R dependency -> migrate to python\n",
    "\n",
    "    feat_flat = [item for sublist in feat.values.tolist() for item in sublist]\n",
    "    feat_temp = robjects.FloatVector(feat_flat)\n",
    "    robjects.globalenv['feat_matrix'] = robjects.r('matrix')(feat_temp)\n",
    "    robjects.globalenv['feat_x'] = robjects.IntVector(feat.shape)\n",
    "    robjects.globalenv['labels_vector'] = robjects.IntVector(labels.tolist())\n",
    "    robjects.globalenv['top'] = num_features\n",
    "    robjects.globalenv['maxBorutaRuns'] = boruta_runs\n",
    "    robjects.r('''\n",
    "        require(rFerns)\n",
    "        require(Boruta)\n",
    "        labels_vector = as.factor(labels_vector)\n",
    "        feat_matrix <- Reshape(feat_matrix, feat_x[1])\n",
    "        feat_data = data.frame(feat_matrix)\n",
    "        colnames(feat_data) <- 1:feat_x[2]\n",
    "        feat_data <- feat_data %>%\n",
    "            mutate('Labels' = labels_vector)\n",
    "        boruta.train <- Boruta(feat_data$Labels ~ ., data= feat_data, doTrace = 0, getImp=getImpFerns, holdHistory = T, maxRuns = maxBorutaRuns)\n",
    "        thr = sort(attStats(boruta.train)$medianImp, decreasing = T)[top]\n",
    "        boruta_signif = rownames(attStats(boruta.train)[attStats(boruta.train)$medianImp >= thr,])\n",
    "            ''')\n",
    "    boruta_signif = robjects.globalenv['boruta_signif']\n",
    "    robjects.r.rm(\"feat_matrix\")\n",
    "    robjects.r.rm(\"labels_vector\")\n",
    "    robjects.r.rm(\"feat_data\")\n",
    "    robjects.r.rm(\"boruta_signif\")\n",
    "    robjects.r.rm(\"thr\")\n",
    "    topx = []\n",
    "    for index in boruta_signif:\n",
    "        t_index=re.sub(\"`\",\"\",index)\n",
    "        topx.append((np.array(feat.values).T)[int(t_index)-1])\n",
    "    topx = np.array(topx)\n",
    "    values = torch.tensor(topx.T, device=device)\n",
    "\n",
    "    return values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(network_name, X_nodes_cat, labels, edge_index, train_valid_idx, test_idx, hid_sizes, learning_rates): \n",
    "\n",
    "    best_valid_loss = np.Inf\n",
    "\n",
    "    # For each hyperparam combo\n",
    "    for hid_size, learning_rate in itertools.product(hid_sizes, learning_rates):  \n",
    "        logger.info(f\"Training model with hidden size {hid_size} and learning rate {learning_rate}\")\n",
    "\n",
    "        # Build a graph where node features have all node feature datatypes concatenated\n",
    "        #  - The edges are still just for this network \n",
    "        data = Data(x=X_nodes_cat, \n",
    "                    edge_index=torch.tensor(edge_index[edge_index.columns[0:2]].transpose().values, device=DEVICE).long(),\n",
    "                    edge_attr=torch.tensor(edge_index[edge_index.columns[2]].transpose().values, device=DEVICE).float(), \n",
    "                    y=labels) \n",
    "\n",
    "        # Split data into training and validation sets\n",
    "        X = data.x[train_valid_idx]\n",
    "        y = data.y[train_valid_idx]\n",
    "\n",
    "        # For each split \n",
    "        rskf = RepeatedStratifiedKFold(n_splits=10, n_repeats=1)\n",
    "        min_valid_losses = []\n",
    "        for train_part, valid_part in rskf.split(X, y):\n",
    "            train_idx = train_valid_idx[train_part]\n",
    "            valid_idx = train_valid_idx[valid_part]\n",
    "\n",
    "            # Get train/valid split \n",
    "            data.train_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "            data.train_mask[train_idx] = True\n",
    "            data.valid_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "            data.valid_mask[valid_idx] = True\n",
    "\n",
    "            # Build model \n",
    "            model = Net(in_size=data.x.shape[1], hid_size=hid_size, out_size=num_classes).to(DEVICE)\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "            criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "            # Train model \n",
    "            min_valid_loss = np.Inf\n",
    "            patience_count = 0\n",
    "\n",
    "            for epoch in range(MAX_EPOCHS):\n",
    "                emb = train_epoch(model, data, optimizer, criterion)\n",
    "                curr_valid_loss, emb = validate_epoch(model, data, criterion)\n",
    "\n",
    "                # Save the min valid loss across all epochs \n",
    "                if curr_valid_loss < min_valid_loss:\n",
    "                    min_valid_loss = curr_valid_loss\n",
    "                    patience_count = 0\n",
    "                else:\n",
    "                    patience_count += 1\n",
    "\n",
    "                # If model isn't learning anything, break\n",
    "                if epoch >= MIN_EPOCHS and patience_count >= PATIENCE:\n",
    "                    break\n",
    "            \n",
    "            min_valid_losses.append(min_valid_loss.item())\n",
    "\n",
    "        curr_min_valid_loss_overall = np.mean(min_valid_losses)            \n",
    "        if curr_min_valid_loss_overall < best_valid_loss:\n",
    "            best_valid_loss = curr_min_valid_loss_overall\n",
    "            best_emb_lr = learning_rate\n",
    "            best_emb_hs = hid_size\n",
    "\n",
    "\n",
    "    # Retrain the model on the entire training set with the best hyperparams\n",
    "    logger.info(f\"Retraining model on full train/valid with hidden size {best_emb_hs} and learning rate {best_emb_lr}\")\n",
    "    data = Data(x=X_nodes_cat, edge_index=torch.tensor(edge_index[edge_index.columns[0:2]].transpose().values, device=DEVICE).long(),\n",
    "                edge_attr=torch.tensor(edge_index[edge_index.columns[2]].transpose().values, device=DEVICE).float(), y=labels) \n",
    "    X = data.x[train_valid_idx]\n",
    "    y = data.y[train_valid_idx]\n",
    "\n",
    "    data.train_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "    data.train_mask[train_valid_idx] = True\n",
    "    data.valid_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "    data.valid_mask[test_idx] = True\n",
    "\n",
    "    model = Net(in_size=data.x.shape[1], hid_size=best_emb_hs, out_size=num_classes).to(DEVICE)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=best_emb_lr)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    selected_emb = None\n",
    "    min_valid_loss = np.Inf\n",
    "    patience_count = 0\n",
    "    history = []\n",
    "                \n",
    "    for epoch in range(MAX_EPOCHS):\n",
    "        emb = train_epoch(model, data, optimizer, criterion)\n",
    "        curr_valid_loss, emb = validate_epoch(model, data, criterion)\n",
    "        history.append(curr_valid_loss)\n",
    "\n",
    "        if curr_valid_loss < min_valid_loss:\n",
    "            min_valid_loss = curr_valid_loss\n",
    "            patience_count = 0\n",
    "            selected_emb = emb\n",
    "        else:\n",
    "            patience_count += 1\n",
    "\n",
    "        if epoch >= MIN_EPOCHS and patience_count >= PATIENCE:\n",
    "            break\n",
    "\n",
    "    logger.info(f\"Final test loss: {min_valid_loss}\")\n",
    "    px.line(history).write_image(os.path.join(OUTPUT_DIR, f\"{network_name}_gcn_history.jpg\"))\n",
    "\n",
    "    # Save the embeddings \n",
    "    emb_file = os.path.join(OUTPUT_DIR, f\"{network_name}_embeddings.pkl\")\n",
    "    with open(emb_file, 'wb') as f:\n",
    "        pickle.dump(selected_emb, f)\n",
    "        pd.DataFrame(selected_emb).to_csv(emb_file[:-4] + '.csv')\n",
    "\n",
    "    return selected_emb\n",
    "\n",
    "def get_classifier(classifier, X_train, y_train): \n",
    "    logger.info(f\"Training classifier {classifier}\")\n",
    "    if classifier == 'MLP':\n",
    "        params = {'hidden_layer_sizes': [(16,), (32,),(64,),(128,),(256,),(512,), (32, 32), (64, 32), (128, 32), (256, 32), (512, 32)]}\n",
    "        search = RandomizedSearchCV(estimator = MLPClassifier(solver = 'adam', activation = 'relu', early_stopping = True), \n",
    "                                    return_train_score = True, scoring = 'f1_macro', \n",
    "                                    param_distributions = params, cv = 4, n_iter = 10, verbose = 0)\n",
    "        search.fit(X_train, y_train)\n",
    "        model = MLPClassifier(solver = 'adam', activation = 'relu', early_stopping = True,\n",
    "                                hidden_layer_sizes = search.best_params_['hidden_layer_sizes'])\n",
    "        \n",
    "    elif classifier == 'XGBoost':\n",
    "        params = {'reg_alpha':range(0,6,1), 'reg_lambda':range(1,5,1),\n",
    "                    'learning_rate':[0, 0.001, 0.01, 1]}\n",
    "        fit_params = {'early_stopping_rounds': 10,\n",
    "                        'eval_metric': 'mlogloss',\n",
    "                        'eval_set': [(X_train, y_train)]}\n",
    "                \n",
    "        search = RandomizedSearchCV(estimator = XGBClassifier(use_label_encoder=False, n_estimators = 1000, \n",
    "                                                                    fit_params = fit_params, objective=\"multi:softprob\", eval_metric = \"mlogloss\", \n",
    "                                                                    verbosity = 0), return_train_score = True, scoring = 'f1_macro',\n",
    "                                        param_distributions = params, cv = 4, n_iter = 10, verbose = 0)\n",
    "        \n",
    "        search.fit(X_train, y_train)\n",
    "        \n",
    "        model = XGBClassifier(use_label_encoder=False, objective=\"multi:softprob\", eval_metric = \"mlogloss\", verbosity = 0,\n",
    "                                n_estimators = 1000, fit_params = fit_params,\n",
    "                                reg_alpha = search.best_params_['reg_alpha'],\n",
    "                                reg_lambda = search.best_params_['reg_lambda'],\n",
    "                                learning_rate = search.best_params_['learning_rate'])\n",
    "                            \n",
    "    elif classifier == 'RF':\n",
    "        max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "        max_depth.append(None)\n",
    "        params = {'n_estimators': [int(x) for x in np.linspace(start = 200, stop = 2000, num = 100)]}\n",
    "        search = RandomizedSearchCV(estimator = RandomForestClassifier(), return_train_score = True,\n",
    "                                    scoring = 'f1_macro', param_distributions = params, cv=4,  n_iter = 10, verbose = 0)\n",
    "        search.fit(X_train, y_train)\n",
    "        model=RandomForestClassifier(n_estimators = search.best_params_['n_estimators'])\n",
    "\n",
    "    elif classifier == 'SVM':\n",
    "        params = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "                    'gamma': [1, 0.1, 0.01, 0.001]}\n",
    "        search = RandomizedSearchCV(SVC(), return_train_score = True,\n",
    "                                    scoring = 'f1_macro', param_distributions = params, cv=4, n_iter = 10, verbose = 0)\n",
    "        search.fit(X_train, y_train)\n",
    "        model=SVC(C = search.best_params_['C'],\n",
    "                    gamma = search.best_params_['gamma'])\n",
    "\n",
    "    logger.info(f'selected parameters = {search.best_params_}')\n",
    "    return model\n",
    "\n",
    "def evaluate_classifier(model, X_train, y_train, X_test, y_test): \n",
    "\n",
    "    metrics = {\n",
    "        'train_acc': [],\n",
    "        'train_wf1': [],\n",
    "        'train_mf1': [],\n",
    "        'test_acc': [],\n",
    "        'test_wf1': [],\n",
    "        'test_mf1': []\n",
    "    }\n",
    "\n",
    "    # Run classifier 10 times and average results\n",
    "    for _ in range(10):\n",
    "        model.fit(X_train,y_train)\n",
    "        predictions = model.predict(X_test)\n",
    "        y_pred = [round(value) for value in predictions]\n",
    "        # preds = model.predict(pd.DataFrame(data.x.numpy()))\n",
    "        tr_predictions = model.predict(X_train)\n",
    "        tr_pred = [round(value) for value in tr_predictions]\n",
    "\n",
    "        metrics['train_acc'].append(round(accuracy_score(y_train, tr_pred), 3))\n",
    "        metrics['train_wf1'].append(round(f1_score(y_train, tr_pred, average='weighted'), 3))\n",
    "        metrics['train_mf1'].append(round(f1_score(y_train, tr_pred, average='macro'), 3))\n",
    "\n",
    "        metrics['test_acc'].append(round(accuracy_score(y_test, y_pred), 3))\n",
    "        metrics['test_wf1'].append(round(f1_score(y_test, y_pred, average='weighted'), 3))\n",
    "        metrics['test_mf1'].append(round(f1_score(y_test, y_pred, average='macro'), 3))\n",
    "\n",
    "    logger.info(\"\\n\".join([f\"{k}: {round(np.mean(v), 3)}+-{round(np.std(v), 3)}\" for k, v in metrics.items()]))\n",
    "\n",
    "    return metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Number of classes: 5\n",
      "INFO:__main__:Training model for network exp\n",
      "INFO:__main__:Training model with hidden size 50 and learning rate 0.01\n",
      "INFO:__main__:Training model with hidden size 50 and learning rate 0.001\n",
      "INFO:__main__:Training model with hidden size 50 and learning rate 0.0001\n",
      "INFO:__main__:Training model with hidden size 100 and learning rate 0.01\n",
      "INFO:__main__:Training model with hidden size 100 and learning rate 0.001\n",
      "INFO:__main__:Training model with hidden size 100 and learning rate 0.0001\n",
      "INFO:__main__:Training model with hidden size 200 and learning rate 0.01\n",
      "INFO:__main__:Training model with hidden size 200 and learning rate 0.001\n",
      "INFO:__main__:Training model with hidden size 200 and learning rate 0.0001\n",
      "INFO:__main__:Retraining model on full train/valid with hidden size 200 and learning rate 0.001\n",
      "INFO:__main__:Final test loss: 0.504334568977356\n",
      "INFO:__main__:Training model for network cna\n",
      "INFO:__main__:Training model with hidden size 50 and learning rate 0.01\n",
      "INFO:__main__:Training model with hidden size 50 and learning rate 0.001\n",
      "INFO:__main__:Training model with hidden size 50 and learning rate 0.0001\n",
      "INFO:__main__:Training model with hidden size 100 and learning rate 0.01\n",
      "INFO:__main__:Training model with hidden size 100 and learning rate 0.001\n",
      "INFO:__main__:Training model with hidden size 100 and learning rate 0.0001\n",
      "INFO:__main__:Training model with hidden size 200 and learning rate 0.01\n",
      "INFO:__main__:Training model with hidden size 200 and learning rate 0.001\n",
      "INFO:__main__:Training model with hidden size 200 and learning rate 0.0001\n",
      "INFO:__main__:Retraining model on full train/valid with hidden size 200 and learning rate 0.0001\n",
      "INFO:__main__:Final test loss: 0.5303795337677002\n"
     ]
    }
   ],
   "source": [
    "labels, nodes, edges = get_data(DATA_DIR, NETWORKS)\n",
    "num_classes = torch.unique(labels).shape[0]\n",
    "logger.info(f'Number of classes: {num_classes}')\n",
    "\n",
    "train_valid_idx, test_idx = train_test_split(np.arange(len(labels)), test_size=0.20, shuffle=True, stratify=labels, random_state=42)\n",
    "\n",
    "# Run feature selection if desired \n",
    "if NODE_FEATURE_SELECTION:\n",
    "    X_nodes = {}\n",
    "    for n in NETWORKS:\n",
    "        # TODO: This doesn't work \n",
    "        X_nodes[n] = select_node_features(nodes[n], NODE_NUM_FEATURES[NETWORKS.index(n)], NUM_BORUTA_RUNS, DEVICE)\n",
    "else:\n",
    "    X_nodes = nodes\n",
    "\n",
    "X_nodes_cat = torch.cat([torch.tensor(v.values).float() for v in X_nodes.values()], dim=1)\n",
    "\n",
    "node_embeddings = {}\n",
    "# For each network \n",
    "for n in NETWORKS: \n",
    "    logger.info(f\"Training model for network {n}\")\n",
    "    edge_index = edges[n]\n",
    "\n",
    "    # Train model\n",
    "    emb = train_model(n, X_nodes_cat, labels, edge_index, train_valid_idx, test_idx, HIDDEN_LAYER_SIZES, LEARNING_RATES)\n",
    "    node_embeddings[n] = emb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Final node embeddings shape: torch.Size([257, 400])\n",
      "INFO:__main__:Final node embeddings shape with raw features: torch.Size([257, 900])\n",
      "INFO:__main__:Shape: (205, 900), (205,), (52, 900), (52,)\n",
      "INFO:__main__:Training classifier MLP\n",
      "INFO:__main__:selected parameters = {'hidden_layer_sizes': (64,)}\n",
      "INFO:__main__:train_acc: 0.922+-0.066\n",
      "train_wf1: 0.921+-0.067\n",
      "train_mf1: 0.866+-0.114\n",
      "test_acc: 0.783+-0.039\n",
      "test_wf1: 0.788+-0.031\n",
      "test_mf1: 0.674+-0.063\n"
     ]
    }
   ],
   "source": [
    "# Use embeddings from all networks for classifier \n",
    "# Concat embeddings from each network \n",
    "node_embeddings_concat = torch.cat([node_embeddings[n] for n in NETWORKS], axis=1)\n",
    "logger.info(f\"Final node embeddings shape: {node_embeddings_concat.shape}\")\n",
    "\n",
    "# Add raw features if desired\n",
    "if ADD_RAW_FEATURES: \n",
    "    X_nodes_cat_raw = torch.cat([torch.tensor(v.values).float() for v in nodes.values()], dim=1)\n",
    "    if NODE_FEATURE_SELECTION: \n",
    "        X_nodes_cat_raw = select_node_features(X_nodes_cat_raw, NODE_NUM_FEATURES[NETWORKS.index(n)], NUM_BORUTA_RUNS, DEVICE)\n",
    "\n",
    "    node_embeddings_concat = torch.cat([node_embeddings_concat, X_nodes_cat_raw], dim=1)\n",
    "    logger.info(f\"Final node embeddings shape with raw features: {node_embeddings_concat.shape}\")\n",
    "\n",
    "data = Data(x=node_embeddings_concat, y=labels)\n",
    "\n",
    "# Split data into training and validation sets\n",
    "data.train_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "data.train_mask[train_valid_idx] = True\n",
    "data.test_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "data.test_mask[test_idx] = True\n",
    "\n",
    "X_train = pd.DataFrame(data.x[data.train_mask])\n",
    "y_train = data.y[data.train_mask].numpy()\n",
    "X_test = pd.DataFrame(data.x[data.test_mask])\n",
    "y_test = data.y[data.test_mask].numpy()\n",
    "\n",
    "logger.info(f\"Shape: {X_train.shape}, {y_train.shape}, {X_test.shape}, {y_test.shape}\")\n",
    "\n",
    "model = get_classifier(CLASSIFIER, X_train, y_train)\n",
    "metrics = evaluate_classifier(model, X_train, y_train, X_test, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multirem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
